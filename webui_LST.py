import gradio as gr
from gradio_client import Client, file
import os
import shutil
import subprocess
import time
import cv2

# GPT-SoVITS API åœ°å€
GPT_SOVITS_API_URL = "http://localhost:9872/"
client = Client(GPT_SOVITS_API_URL)

# Wav2Lip ç›¸å…³è·¯å¾„
wav2lip_env_path = "E:/Project/Digital_Human/Wav2Lip/env"
wav2lip_script_path = "E:/Project/Digital_Human/Wav2Lip/inference.py"
wav2lip_checkpoint_path = "E:/Project/Digital_Human/Wav2Lip/models/wav2lip_gan.pth"
input_video_dir = "E:/Project/Digital_Human/input"
output_video_path = "E:/Project/Digital_Human/Wav2Lip/results/result_voice.mp4"

# ç¡®ä¿ input æ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs(input_video_dir, exist_ok=True)

# è·å–å¯ç”¨çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨
def get_video_files():
    return [f for f in os.listdir(input_video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]

# å½•åˆ¶è§†é¢‘åŠŸèƒ½ï¼ˆå½•åˆ¶å®Œæˆååˆ·æ–°ä¸‹æ‹‰åˆ—è¡¨ï¼‰
def record_video():
    cap = cv2.VideoCapture(0)
    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    video_name = f"recorded_{int(time.time())}.avi"
    video_path = os.path.join(input_video_dir, video_name)
    out = cv2.VideoWriter(video_path, fourcc, 20.0, (640, 480))

    print("æ­£åœ¨å½•åˆ¶è§†é¢‘, æŒ‰ 'q' é€€å‡º...")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        out.write(frame)
        cv2.imshow('Recording', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    
    print(f"å½•åˆ¶å®Œæˆ: {video_path}")

    return gr.update(choices=get_video_files(), value=video_name)

# è·å–å¯ç”¨æ¨¡å‹
def get_available_models():
    result = client.predict(api_name="/change_choices")
    if not isinstance(result, tuple) or len(result) < 2:
        raise ValueError(f"API è¿”å›æ ¼å¼ä¸æ­£ç¡®: {result}")

    sovits_models = [model[0] for model in result[0]["choices"]]
    gpt_models = [model[0] for model in result[1]["choices"]]

    print("å¯ç”¨çš„ SoVITS è¯­éŸ³æ¨¡å‹:", sovits_models)
    print("å¯ç”¨çš„ GPT è¯­éŸ³æ¨¡å‹:", gpt_models)

    return sovits_models, gpt_models

# åˆ‡æ¢æ¨¡å‹
def change_models(sovits_model, gpt_model):
    client.predict(
        sovits_path=sovits_model,
        prompt_language="ä¸­æ–‡",
        text_language="ä¸­æ–‡",
        api_name="/change_sovits_weights"
    )
    print(f"å·²åˆ‡æ¢ SoVITS è¯­éŸ³æ¨¡å‹: {sovits_model}")

    client.predict(
        gpt_path=gpt_model,
        api_name="/change_gpt_weights"
    )
    print(f"å·²åˆ‡æ¢ GPT è¯­éŸ³æ¨¡å‹: {gpt_model}")

# ç”Ÿæˆè¯­éŸ³å¹¶è°ƒç”¨ Wav2Lip ç”Ÿæˆè§†é¢‘
def generate_speech(ref_audio_path, text, sovits_model, gpt_model, prompt_text, prompt_language, how_to_cut, top_k, top_p, temperature, ref_free, speed, if_freeze, sample_steps, input_video):
    change_models(sovits_model, gpt_model)

    print("æ­£åœ¨å‘ GPT-SoVITS å‘é€ TTS è¯·æ±‚...\n")
    
    result = client.predict(
        ref_wav_path=file(ref_audio_path),
        prompt_text=prompt_text,
        prompt_language=prompt_language,
        text=text,
        text_language="ä¸­æ–‡",
        how_to_cut=how_to_cut,
        top_k=top_k,
        top_p=top_p,
        temperature=temperature,
        ref_free=ref_free,
        speed=speed,
        if_freeze=if_freeze,
        inp_refs=None,
        sample_steps=sample_steps,
        api_name="/get_tts_wav"
    )

    output_audio_path = os.path.join("output", "generated_audio.wav")
    shutil.copy(result, output_audio_path)
    print(f"è¯­éŸ³åˆæˆå®Œæˆ: {output_audio_path}")

    input_video_path = os.path.join(input_video_dir, input_video)

    if not os.path.exists(input_video_path):
        raise FileNotFoundError(f"è¾“å…¥è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_video_path}")

    if os.path.exists(output_video_path):
        os.remove(output_video_path)

    wav2lip_command = [
        "conda", "run", "-p", wav2lip_env_path,
        "python", wav2lip_script_path,
        "--checkpoint_path", wav2lip_checkpoint_path,
        "--face", input_video_path,
        "--audio", output_audio_path,
        "--outfile", output_video_path
    ]

    print("æ­£åœ¨è¿è¡Œ Wav2Lip ...")
    subprocess.run(wav2lip_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1) 

    wait_time = 0
    while not os.path.exists(output_video_path):
        time.sleep(1)
        wait_time += 1
        print(f"ç­‰å¾…è§†é¢‘ç”Ÿæˆ... {wait_time}s")
        if wait_time > 300:
            print("è¶…æ—¶: Wav2Lip å¯èƒ½ç”Ÿæˆå¤±è´¥")
            break

    if os.path.exists(output_video_path):
        print(f"Wav2Lip è§†é¢‘ç”ŸæˆæˆåŠŸ: {output_video_path}")
    else:
        print(f"Wav2Lip ç”Ÿæˆå¤±è´¥, æ–‡ä»¶ {output_video_path} æœªæ‰¾åˆ°")

    return output_audio_path, output_video_path

# è·å–å¯ç”¨æ¨¡å‹å¹¶åˆå§‹åŒ–
sovits_models, gpt_models = get_available_models()

# Gradio ç•Œé¢
with gr.Blocks() as demo:
    
    with gr.Row():
        gr.Text("è¯·æ³¨æ„å½•åˆ¶çš„è§†é¢‘éœ€åŒ…å«æ¸…æ™°äººè„¸å’Œè¯´è¯åœºæ™¯")
        
    with gr.Row():
        record_button = gr.Button("ğŸ¥ å½•åˆ¶è§†é¢‘")
        input_video_dropdown = gr.Dropdown(get_video_files(), label="é€‰æ‹©è¾“å…¥è§†é¢‘")

    with gr.Row():
        sovits_model_dropdown = gr.Dropdown(sovits_models, label="é€‰æ‹© SoVITS æ¨¡å‹")
        gpt_model_dropdown = gr.Dropdown(gpt_models, label="é€‰æ‹© GPT æ¨¡å‹")
    
    with gr.Row():
        ref_audio_input = gr.Audio(label="ä¸Šä¼ å‚è€ƒéŸ³é¢‘", type="filepath")
        text_input = gr.Textbox(label="è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬", placeholder="è¯·è¾“å…¥æ–‡æœ¬")

    with gr.Row():
        prompt_text_input = gr.Textbox(label="è¾“å…¥æç¤ºæ–‡æœ¬", value="ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„æ•°å­—äººåŠ©æ‰‹ï¼")
        ref_free_input = gr.Checkbox(label="å¼€å¯æ— å‚è€ƒæ–‡æœ¬æ¨¡å¼", value=False)
        prompt_language_input = gr.Dropdown(["ä¸­æ–‡", "è‹±æ–‡"], label="é€‰æ‹©æç¤ºè¯­è¨€", value="ä¸­æ–‡")
        how_to_cut_input = gr.Textbox(label="å¦‚ä½•åˆ‡å‰²æ–‡æœ¬", value="å‡‘å››å¥ä¸€åˆ‡")
        top_k_input = gr.Slider(minimum=1, maximum=50, label="Top-k", value=15)
        top_p_input = gr.Slider(minimum=0.0, maximum=1.0, label="Top-p", value=1.0)
        temperature_input = gr.Slider(minimum=0.0, maximum=2.0, label="temperature", value=1.0)
    
    with gr.Row():
        if_freeze_input = gr.Checkbox(label="æ˜¯å¦ç›´æ¥å¯¹ä¸Šæ¬¡åˆæˆç»“æœè°ƒæ•´è¯­é€Ÿå’ŒéŸ³è‰²", value=False)
        speed_input = gr.Slider(minimum=0.5, maximum=2.0, label="è¯­é€Ÿ", value=1.0)
        sample_steps_input = gr.Slider(minimum=1, maximum=50, label="é‡‡æ ·æ­¥æ•°", value=8)

    with gr.Row():
        generate_button = gr.Button("ç”Ÿæˆæ•°å­—äººè¯­éŸ³ä¸è§†é¢‘")
    
    with gr.Row():
        output_audio = gr.Audio(label="ç”Ÿæˆçš„éŸ³é¢‘")
        output_video = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")
    record_button.click(record_video, outputs=input_video_dropdown)
    generate_button.click(
        generate_speech,
        inputs=[ref_audio_input, text_input, sovits_model_dropdown, gpt_model_dropdown, prompt_text_input, prompt_language_input, how_to_cut_input, top_k_input, top_p_input, temperature_input, ref_free_input, speed_input, if_freeze_input, sample_steps_input, input_video_dropdown],
        outputs=[output_audio, output_video]
    )

demo.launch()
